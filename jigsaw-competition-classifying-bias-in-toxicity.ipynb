{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from textblob import TextBlob\n",
    "import gensim\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.initializers import *\n",
    "from keras.constraints import *\n",
    "from keras.regularizers import *\n",
    "from keras.activations import *\n",
    "from keras.optimizers import *\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_COL = 'comment_text'\n",
    "EMB_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
    "MAXLEN = 220\n",
    "MAX_FEATURES = 100000\n",
    "EMBED_SIZE = 300\n",
    "BATCH_SIZE = 2048\n",
    "NUM_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/cpmpml/spell-checker-using-word2vec\n",
    "# spell_model = gensim.models.KeyedVectors.load_word2vec_format('../input/quora-insincere-questions-classification/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec')\n",
    "# words = spell_model.index2word\n",
    "# w_rank = {}\n",
    "\n",
    "# for i,word in enumerate(words):\n",
    "    # w_rank[word] = i\n",
    "# WORDS = w_rank\n",
    "\n",
    "# Use fast text as vocabulary\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def P(word): \n",
    "    \"Probability of `word`.\"\n",
    "    # use inverse of rank as proxy\n",
    "    # returns 0 if the word isn't in the dictionary\n",
    "    return - WORDS.get(word, 0)\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "def singlify(word):\n",
    "    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(function, list):\n",
    "    \"\"\" Returns the first item in the list for which function(item) is True, None otherwise.\n",
    "    \"\"\"\n",
    "    for item in list:\n",
    "        if function(item):\n",
    "            return item\n",
    "\n",
    "### MOOD ###########################################################################################\n",
    "# Functions take Sentence objects, see pattern.text.tree.Sentence and pattern.text.parsetree().\n",
    "\n",
    "INDICATIVE = \"indicative\"  # They went for a walk.\n",
    "IMPERATIVE = \"imperative\"  # Let's go for a walk!\n",
    "CONDITIONAL = \"conditional\" # It might be nice to go for a walk when it stops raining.\n",
    "SUBJUNCTIVE = \"subjunctive\" # It would be nice to go for a walk sometime.\n",
    "\n",
    "\n",
    "def s(word):\n",
    "    return word.string.lower()\n",
    "\n",
    "\n",
    "def join(words):\n",
    "    return \" \".join([w.string.lower() for w in words])\n",
    "\n",
    "\n",
    "def question(sentence):\n",
    "    return len(sentence) > 0 and sentence[-1].string == \"?\"\n",
    "\n",
    "\n",
    "def verb(word):\n",
    "    return word.type.startswith((\"VB\", \"MD\")) and (word.chunk is None or word.chunk.type.endswith(\"VP\"))\n",
    "\n",
    "\n",
    "def verbs(sentence, i=0, j=None):\n",
    "    return [w for w in sentence[i:j or len(sentence)] if verb(w)]\n",
    "\n",
    "\n",
    "def imperative(sentence, **kwargs):\n",
    "    \"\"\" The imperative mood is used to give orders, commands, warnings, instructions, \n",
    "        or to make requests (if used with \"please\").\n",
    "        It is marked by the infinitive form of the verb, without \"to\":\n",
    "        \"For goodness sake, just stop it!\"\n",
    "    \"\"\"\n",
    "    S = sentence\n",
    "    if not (hasattr(S, \"words\") and hasattr(S, \"parse_token\")):\n",
    "        raise TypeError(\"%s object is not a parsed Sentence\" % repr(S.__class__.__name__))\n",
    "    if question(S):\n",
    "        return False\n",
    "    if S.subjects and s(S.subjects[0]) not in (\"you\", \"yourself\"):\n",
    "        # The subject can only identify as \"you\" (2sg): \"Control yourself!\".\n",
    "        return False\n",
    "    r = s(S).rstrip(\" .!\")\n",
    "    for cc in (\"if\", \"assuming\", \"provided that\", \"given that\"):\n",
    "        # A conjunction can also indicate conditional mood.\n",
    "        if cc + \" \" in r:\n",
    "            return False\n",
    "    for i, w in enumerate(S):\n",
    "        if verb(w):\n",
    "            if s(w) in (\"do\", \"let\") and w == verbs(S)[0]:\n",
    "                # \"Do your homework!\"\n",
    "                return True\n",
    "            if s(w) in (\"do\", \"let\"):\n",
    "                # \"Let's not argue.\"\n",
    "                continue\n",
    "            if s(w) in (\"would\", \"should\", \"'d\", \"could\", \"can\", \"may\", \"might\"):\n",
    "                # \"You should leave.\" => conditional.\n",
    "                return False\n",
    "            if s(w) in (\"will\", \"shall\") and i > 0 and s(S[i - 1]) == \"you\" and not verbs(S, 0, i):\n",
    "                # \"You will eat your dinner.\"\n",
    "                continue\n",
    "            if w.type == \"VB\" and (i == 0 or s(S[i - 1]) != \"to\"):\n",
    "                # \"Come here!\"\n",
    "                return True\n",
    "            # Break on any other verb form.\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "#from __init__ import parse, Sentence\n",
    "#\n",
    "#for str in (\n",
    "#  \"Do your homework!\",                   # True\n",
    "#  \"Do whatever you want.\",               # True\n",
    "#  \"Do not listen to me.\",                # True\n",
    "#  \"Do it if you think it is necessary.\", # False\n",
    "#  \"Turn that off, will you.\",            # True\n",
    "#  \"Let's help him.\",                     # True\n",
    "#  \"Help me!\",                            # True\n",
    "#  \"You will help me.\",                   # True\n",
    "#  \"I hope you will help me.\",            # False\n",
    "#  \"I can help you.\",                     # False\n",
    "#  \"I can help you if you let me.\"):      # False\n",
    "#    print str\n",
    "#    print parse(str)\n",
    "#    print imperative(Sentence(parse(str)))\n",
    "#    print\n",
    "\n",
    "\n",
    "def conditional(sentence, predictive=True, **kwargs):\n",
    "    \"\"\" The conditional mood is used to talk about possible or imaginary situations.\n",
    "        It is marked by the infinitive form of the verb, preceded by would/could/should:\n",
    "        \"we should be going\", \"we could have stayed longer\".\n",
    "        With predictive=False, sentences with will/shall need an explicit if/when/once-clause:\n",
    "        - \"I will help you\" => predictive.\n",
    "        - \"I will help you if you pay me\" => speculative.\n",
    "        Sentences with can/may always need an explicit if-clause.\n",
    "    \"\"\"\n",
    "    S = sentence\n",
    "    if not (hasattr(S, \"words\") and hasattr(S, \"parse_token\")):\n",
    "        raise TypeError(\"%s object is not a parsed Sentence\" % repr(S.__class__.__name__))\n",
    "    if question(S):\n",
    "        return False\n",
    "    i = find(lambda w: s(w) == \"were\", S)\n",
    "    i = i and i.index or 0\n",
    "    if i > 0 and (s(S[i - 1]) in (\"i\", \"it\", \"he\", \"she\") or S[i - 1].type == \"NN\"):\n",
    "        # \"As if it were summer already.\" => subjunctive (wish).\n",
    "        return False\n",
    "    for i, w in enumerate(S):\n",
    "        if w.type == \"MD\":\n",
    "            if s(w) == \"ought\" and i < len(S) and s(S[i + 1]) == \"to\":\n",
    "                # \"I ought to help you.\"\n",
    "                return True\n",
    "            if s(w) in (\"would\", \"should\", \"'d\", \"could\", \"might\"):\n",
    "                # \"I could help you.\"\n",
    "                return True\n",
    "            if s(w) in (\"will\", \"shall\", \"'ll\") and i > 0 and s(S[i - 1]) == \"you\" and not verbs(S, 0, i):\n",
    "                # \"You will help me.\" => imperative.\n",
    "                return False\n",
    "            if s(w) in (\"will\", \"shall\", \"'ll\") and predictive:\n",
    "                # \"I will help you.\" => predictive.\n",
    "                return True\n",
    "            if s(w) in (\"will\", \"shall\", \"'ll\", \"can\", \"may\"):\n",
    "                # \"I will help you when I get back.\" => speculative.\n",
    "                r = s(S).rstrip(\" .!\")\n",
    "                for cc in (\"if\", \"when\", \"once\", \"as soon as\", \"assuming\", \"provided that\", \"given that\"):\n",
    "                    if cc + \" \" in r:\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "#from __init__ import parse, Sentence\n",
    "#\n",
    "#for str in (\n",
    "#  \"We ought to help him.\",          # True\n",
    "#  \"We could help him.\",             # True\n",
    "#  \"I will help you.\",               # True\n",
    "#  \"You will help me.\",              # False (imperative)\n",
    "#  \"I hope you will help me.\",       # True (predictive)\n",
    "#  \"I can help you.\",                # False\n",
    "#  \"I can help you if you let me.\"): # True\n",
    "#    print str\n",
    "#    print parse(str)\n",
    "#    print conditional(Sentence(parse(str)))\n",
    "#    print\n",
    "\n",
    "subjunctive1 = [\n",
    "    \"advise\", \"ask\", \"command\", \"demand\", \"desire\", \"insist\",\n",
    "    \"propose\", \"recommend\", \"request\", \"suggest\", \"urge\"]\n",
    "subjunctive2 = [\n",
    "    \"best\", \"crucial\", \"desirable\", \"essential\", \"imperative\",\n",
    "    \"important\", \"recommended\", \"urgent\", \"vital\"]\n",
    "\n",
    "for w in list(subjunctive1): # Inflect.\n",
    "    subjunctive1.append(w + \"s\")\n",
    "    subjunctive1.append(w.rstrip(\"e\") + \"ed\")\n",
    "\n",
    "\n",
    "def subjunctive(sentence, classical=True, **kwargs):\n",
    "    \"\"\" The subjunctive mood is a classical mood used to express a wish, judgment or opinion.\n",
    "        It is marked by the verb wish/were, or infinitive form of a verb\n",
    "        preceded by an \"it is\"-statement:\n",
    "        \"It is recommended that he bring his own computer.\"\n",
    "    \"\"\"\n",
    "    S = sentence\n",
    "    if not (hasattr(S, \"words\") and hasattr(S, \"parse_token\")):\n",
    "        raise TypeError(\"%s object is not a parsed Sentence\" % repr(S.__class__.__name__))\n",
    "    if question(S):\n",
    "        return False\n",
    "    for i, w in enumerate(S):\n",
    "        b = False\n",
    "        if w.type.startswith(\"VB\"):\n",
    "            if s(w).startswith(\"wish\"):\n",
    "                # \"I wish I knew.\"\n",
    "                return True\n",
    "            if s(w) == \"hope\" and i > 0 and s(S[i - 1]) in (\"i\", \"we\"):\n",
    "                # \"I hope ...\"\n",
    "                return True\n",
    "            if s(w) == \"were\" and i > 0 and (s(S[i - 1]) in (\"i\", \"it\", \"he\", \"she\") or S[i - 1].type == \"NN\"):\n",
    "                # \"It is as though she were here.\" => counterfactual.\n",
    "                return True\n",
    "            if s(w) in subjunctive1:\n",
    "                # \"I propose that you be on time.\"\n",
    "                b = True\n",
    "            elif s(w) == \"is\" and 0 < i < len(S) - 1 and s(S[i - 1]) == \"it\" \\\n",
    "             and s(S[i + 1]) in subjunctive2:\n",
    "                # \"It is important that you be there.\" => but you aren't (yet).\n",
    "                b = True\n",
    "            elif s(w) == \"is\" and 0 < i < len(S) - 3 and s(S[i - 1]) == \"it\" \\\n",
    "             and s(S[i + 2]) in (\"good\", \"bad\") and s(S[i + 3]) == \"idea\":\n",
    "                # \"It is a good idea that you be there.\"\n",
    "                b = True\n",
    "        if b:\n",
    "            # With classical=False, \"It is important that you are there.\" passes.\n",
    "            # This is actually an informal error: it states a fact, not a wish.\n",
    "            v = find(lambda w: w.type.startswith(\"VB\"), S[i + 1:])\n",
    "            if v and classical is True and v and v.type == \"VB\":\n",
    "                return True\n",
    "            if v and classical is False:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "#from __init__ import parse, Sentence\n",
    "#\n",
    "#for str in (\n",
    "#  \"I wouldn't do that if I were you.\", # True\n",
    "#  \"I wish I knew.\",                    # True\n",
    "#  \"I propose that you be on time.\",    # True\n",
    "#  \"It is a bad idea to be late.\",      # True\n",
    "#  \"I will be dead.\"):                  # False, predictive\n",
    "#    print str\n",
    "#    print parse(str)\n",
    "#    print subjunctive(Sentence(parse(str)))\n",
    "#    print\n",
    "\n",
    "\n",
    "def negated(sentence, negative=(\"not\", \"n't\", \"never\")):\n",
    "    if hasattr(sentence, \"string\"):\n",
    "        # Sentence object => string.\n",
    "        sentence = sentence.string\n",
    "    S = \" %s \" % (sentence).strip(\".?!\").lower()\n",
    "    for w in negative:\n",
    "        if \" %s \" % w in S:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def mood(sentence, **kwargs):\n",
    "    \"\"\" Returns IMPERATIVE (command), CONDITIONAL (possibility), SUBJUNCTIVE (wish) or INDICATIVE (fact).\n",
    "    \"\"\"\n",
    "    if isinstance(sentence, str):\n",
    "        try:\n",
    "            # A Sentence is expected but a string given.\n",
    "            # Attempt to parse the string on-the-fly.\n",
    "            from pattern.en import parse, Sentence\n",
    "            sentence = Sentence(parse(sentence))\n",
    "        except ImportError:\n",
    "            pass\n",
    "    if imperative(sentence, **kwargs):\n",
    "        return IMPERATIVE\n",
    "    if conditional(sentence, **kwargs):\n",
    "        return CONDITIONAL\n",
    "    if subjunctive(sentence, **kwargs):\n",
    "        return SUBJUNCTIVE\n",
    "    else:\n",
    "        return INDICATIVE\n",
    "\n",
    "### MODALITY #######################################################################################\n",
    "# Functions take Sentence objects, see pattern.text.tree.Sentence and pattern.text.parsetree().\n",
    "\n",
    "\n",
    "def d(*args):\n",
    "    return dict.fromkeys(args, True)\n",
    "\n",
    "AUXILLARY = {\n",
    "      \"be\": [\"be\", \"am\", \"m\", \"are\", \"is\", \"being\", \"was\", \"were\" \"been\"],\n",
    "     \"can\": [\"can\", \"ca\", \"could\"],\n",
    "    \"dare\": [\"dare\", \"dares\", \"daring\", \"dared\"],\n",
    "      \"do\": [\"do\", \"does\", \"doing\", \"did\", \"done\"],\n",
    "    \"have\": [\"have\", \"ve\", \"has\", \"having\", \"had\"],\n",
    "     \"may\": [\"may\", \"might\"],\n",
    "    \"must\": [\"must\"],\n",
    "    \"need\": [\"need\", \"needs\", \"needing\", \"needed\"],\n",
    "   \"ought\": [\"ought\"],\n",
    "   \"shall\": [\"shall\", \"sha\"],\n",
    "    \"will\": [\"will\", \"ll\", \"wo\", \"willing\", \"would\", \"d\"]\n",
    "}\n",
    "\n",
    "MODIFIERS = (\"fully\", \"highly\", \"most\", \"much\", \"strongly\", \"very\")\n",
    "\n",
    "EPISTEMIC = \"epistemic\" # Expresses degree of possiblity.\n",
    "\n",
    "# -1.00 = NEGATIVE\n",
    "# -0.75 = NEGATIVE, with slight doubts\n",
    "# -0.50 = NEGATIVE, with doubts\n",
    "# -0.25 = NEUTRAL, slightly negative\n",
    "# +0.00 = NEUTRAL\n",
    "# +0.25 = NEUTRAL, slightly positive\n",
    "# +0.50 = POSITIVE, with doubts\n",
    "# +0.75 = POSITIVE, with slight doubts\n",
    "# +1.00 = POSITIVE\n",
    "\n",
    "epistemic_MD = { # would => could => can => should => shall => will => must\n",
    "    -1.00: d(),\n",
    "    -0.75: d(),\n",
    "    -0.50: d(\"would\"),\n",
    "    -0.25: d(\"could\", \"dare\", \"might\"),\n",
    "     0.00: d(\"can\", \"ca\", \"may\"),\n",
    "    +0.25: d(\"ought\", \"should\"),\n",
    "    +0.50: d(\"shall\", \"sha\"),\n",
    "    +0.75: d(\"will\", \"'ll\", \"wo\"),\n",
    "    +1.00: d(\"have\", \"has\", \"must\", \"need\"),\n",
    "}\n",
    "\n",
    "epistemic_VB = { # wish => feel => believe => seem => think => know => prove + THAT\n",
    "    -1.00: d(),\n",
    "    -0.75: d(),\n",
    "    -0.50: d(\"dispute\", \"disputed\", \"doubt\", \"question\"),\n",
    "    -0.25: d(\"hope\", \"want\", \"wish\"),\n",
    "     0.00: d(\"guess\", \"imagine\", \"seek\"),\n",
    "    +0.25: d(\"appear\", \"bet\", \"feel\", \"hear\", \"rumor\", \"rumour\", \"say\", \"said\", \"seem\", \"seemed\",\n",
    "             \"sense\", \"speculate\", \"suspect\", \"suppose\", \"wager\"),\n",
    "    +0.50: d(\"allude\", \"anticipate\", \"assume\", \"claim\", \"claimed\", \"believe\", \"believed\",\n",
    "             \"conjecture\", \"consider\", \"considered\", \"decide\", \"expect\", \"find\", \"found\",\n",
    "             \"hypothesize\", \"imply\", \"indicate\", \"infer\", \"postulate\", \"predict\", \"presume\",\n",
    "             \"propose\", \"report\", \"reported\", \"suggest\", \"suggested\", \"tend\",\n",
    "             \"think\", \"thought\"),\n",
    "    +0.75: d(\"know\", \"known\", \"look\", \"see\", \"show\", \"shown\"),\n",
    "    +1.00: d(\"certify\", \"demonstrate\", \"prove\", \"proven\", \"verify\"),\n",
    "}\n",
    "\n",
    "epistemic_RB = { # unlikely => supposedly => maybe => probably => usually => clearly => definitely\n",
    "    -1.00: d(\"impossibly\"),\n",
    "    -0.75: d(\"hardly\"),\n",
    "    -0.50: d(\"presumptively\", \"rarely\", \"scarcely\", \"seldomly\", \"uncertainly\", \"unlikely\"),\n",
    "    -0.25: d(\"almost\", \"allegedly\", \"debatably\", \"nearly\", \"presumably\", \"purportedly\", \"reportedly\",\n",
    "             \"reputedly\", \"rumoredly\", \"rumouredly\", \"supposedly\"),\n",
    "     0.00: d(\"barely\", \"hypothetically\", \"maybe\", \"occasionally\", \"perhaps\", \"possibly\", \"putatively\",\n",
    "             \"sometimes\", \"sporadically\", \"traditionally\", \"widely\"),\n",
    "    +0.25: d(\"admittedly\", \"apparently\", \"arguably\", \"believably\", \"conceivably\", \"feasibly\", \"fairly\",\n",
    "             \"hopefully\", \"likely\", \"ostensibly\", \"potentially\", \"probably\", \"quite\", \"seemingly\"),\n",
    "    +0.50: d(\"commonly\", \"credibly\", \"defendably\", \"defensibly\", \"effectively\", \"frequently\",\n",
    "             \"generally\", \"largely\", \"mostly\", \"normally\", \"noticeably\", \"often\", \"plausibly\",\n",
    "             \"reasonably\", \"regularly\", \"relatively\", \"typically\", \"usually\"),\n",
    "    +0.75: d(\"assuredly\", \"certainly\", \"clearly\", \"doubtless\", \"evidently\", \"evitably\", \"manifestly\",\n",
    "             \"necessarily\", \"nevertheless\", \"observably\", \"ostensively\", \"patently\", \"plainly\",\n",
    "             \"positively\", \"really\", \"surely\", \"truly\", \"undoubtably\", \"undoubtedly\", \"verifiably\"),\n",
    "    +1.00: d(\"absolutely\", \"always\", \"definitely\", \"incontestably\", \"indisputably\", \"indubitably\",\n",
    "             \"ineluctably\", \"inescapably\", \"inevitably\", \"invariably\", \"obviously\", \"unarguably\",\n",
    "             \"unavoidably\", \"undeniably\", \"unquestionably\")\n",
    "}\n",
    "\n",
    "epistemic_JJ = {\n",
    "    -1.00: d(\"absurd\", \"prepostoreous\", \"ridiculous\"),\n",
    "    -0.75: d(\"inconceivable\", \"unthinkable\"),\n",
    "    -0.50: d(\"misleading\", \"scant\", \"unlikely\", \"unreliable\"),\n",
    "    -0.25: d(\"customer-centric\", \"doubtful\", \"ever\", \"ill-defined, \"\"inadequate\", \"late\",\n",
    "             \"uncertain\", \"unclear\", \"unrealistic\", \"unspecified\", \"unsure\", \"wild\"),\n",
    "     0.00: d(\"dynamic\", \"possible\", \"unknown\"),\n",
    "    +0.25: d(\"according\", \"creative\", \"likely\", \"local\", \"innovative\", \"interesting\",\n",
    "             \"potential\", \"probable\", \"several\", \"some\", \"talented\", \"viable\"),\n",
    "    +0.50: d(\"certain\", \"generally\", \"many\", \"notable\", \"numerous\", \"performance-oriented\",\n",
    "             \"promising\", \"putative\", \"well-known\"),\n",
    "    +0.75: d(\"concrete\", \"credible\", \"famous\", \"important\", \"major\", \"necessary\", \"original\",\n",
    "             \"positive\", \"significant\", \"real\", \"robust\", \"substantial\", \"sure\"),\n",
    "    +1.00: d(\"confirmed\", \"definite\", \"prime\", \"undisputable\"),\n",
    "}\n",
    "\n",
    "epistemic_NN = {\n",
    "    -1.00: d(\"fantasy\", \"fiction\", \"lie\", \"myth\", \"nonsense\"),\n",
    "    -0.75: d(\"controversy\"),\n",
    "    -0.50: d(\"criticism\", \"debate\", \"doubt\"),\n",
    "    -0.25: d(\"belief\", \"chance\", \"faith\", \"luck\", \"perception\", \"speculation\"),\n",
    "     0.00: d(\"challenge\", \"guess\", \"feeling\", \"hunch\", \"opinion\", \"possibility\", \"question\"),\n",
    "    +0.25: d(\"assumption\", \"expectation\", \"hypothesis\", \"notion\", \"others\", \"team\"),\n",
    "    +0.50: d(\"example\", \"proces\", \"theory\"),\n",
    "    +0.75: d(\"conclusion\", \"data\", \"evidence\", \"majority\", \"proof\", \"symptom\", \"symptoms\"),\n",
    "    +1.00: d(\"fact\", \"truth\", \"power\"),\n",
    "}\n",
    "\n",
    "epistemic_CC_DT_IN = {\n",
    "     0.00: d(\"either\", \"whether\"),\n",
    "    +0.25: d(\"however\", \"some\"),\n",
    "    +1.00: d(\"despite\")\n",
    "}\n",
    "\n",
    "epistemic_PRP = {\n",
    "    +0.25: d(\"I\", \"my\"),\n",
    "    +0.50: d(\"our\"),\n",
    "    +0.75: d(\"we\")\n",
    "}\n",
    "\n",
    "epistemic_weaseling = {\n",
    "    -0.75: d(\"popular belief\"),\n",
    "    -0.50: d(\"but that\", \"but this\", \"have sought\", \"might have\", \"seems to\"),\n",
    "    -0.25: d(\"may also\", \"may be\", \"may have\", \"may have been\", \"some have\", \"sort of\"),\n",
    "    +0.00: d(\"been argued\", \"believed to\", \"considered to\", \"claimed to\", \"is considered\", \"is possible\",\n",
    "             \"overall solutions\", \"regarded as\", \"said to\"),\n",
    "    +0.25: d(\"a number of\", \"in some\", \"one of\", \"some of\",\n",
    "             \"many modern\", \"many people\", \"most people\", \"some people\", \"some cases\", \"some studies\",\n",
    "             \"scientists\", \"researchers\"),\n",
    "    +0.50: d(\"in several\", \"is likely\", \"many of\", \"many other\", \"of many\", \"of the most\", \"such as\",\n",
    "             \"several reasons\", \"several studies\", \"several universities\", \"wide range\"),\n",
    "    +0.75: d(\"almost always\", \"and many\", \"and some\", \"around the world\", \"by many\", \"in many\", \"in order to\",\n",
    "             \"most likely\"),\n",
    "    +1.00: d(\"i.e.\", \"'s most\", \"of course\", \"There are\", \"without doubt\"),\n",
    "}\n",
    "\n",
    "\n",
    "def modality(sentence, type=EPISTEMIC):\n",
    "    \"\"\" Returns the sentence's modality as a weight between -1.0 and +1.0.\n",
    "        Currently, the only type implemented is EPISTEMIC.\n",
    "        Epistemic modality is used to express possibility (i.e. how truthful is what is being said).\n",
    "    \"\"\"\n",
    "    if isinstance(sentence, str):\n",
    "        try:\n",
    "            # A Sentence is expected but a string given.\n",
    "            # Attempt to parse the string on-the-fly.\n",
    "            from pattern.en import parse, Sentence\n",
    "            sentence = Sentence(parse(sentence))\n",
    "        except ImportError:\n",
    "            pass\n",
    "    S, n, m = sentence, 0.0, 0\n",
    "    if not (hasattr(S, \"words\") and hasattr(S, \"parse_token\")):\n",
    "        raise TypeError(\"%s object is not a parsed Sentence\" % repr(S.__class__.__name__))\n",
    "    if type == EPISTEMIC:\n",
    "        r = S.string.rstrip(\" .!\")\n",
    "        for k, v in epistemic_weaseling.items():\n",
    "            for phrase in v:\n",
    "                if phrase in r:\n",
    "                    n += k\n",
    "                    m += 2\n",
    "        for i, w in enumerate(S.words):\n",
    "            for type, dict, weight in (\n",
    "              (  \"MD\", epistemic_MD, 4),\n",
    "              (  \"VB\", epistemic_VB, 2),\n",
    "              (  \"RB\", epistemic_RB, 2),\n",
    "              (  \"JJ\", epistemic_JJ, 1),\n",
    "              (  \"NN\", epistemic_NN, 1),\n",
    "              (  \"CC\", epistemic_CC_DT_IN, 1),\n",
    "              (  \"DT\", epistemic_CC_DT_IN, 1),\n",
    "              (  \"IN\", epistemic_CC_DT_IN, 1),\n",
    "              (\"PRP\" , epistemic_PRP, 1),\n",
    "              (\"PRP$\", epistemic_PRP, 1),\n",
    "              ( \"WP\" , epistemic_PRP, 1)):\n",
    "                # \"likely\" => weight 1, \"very likely\" => weight 2\n",
    "                if i > 0 and s(S[i - 1]) in MODIFIERS:\n",
    "                    weight += 1\n",
    "                # likely\" => score 0.25 (neutral inclining towards positive).\n",
    "                if w.type and w.type.startswith(type):\n",
    "                    for k, v in dict.items():\n",
    "                        # Prefer lemmata.\n",
    "                        if (w.lemma or s(w)) in v:\n",
    "                            # Reverse score for negated terms.\n",
    "                            if i > 0 and s(S[i - 1]) in (\"not\", \"n't\", \"never\", \"without\"):\n",
    "                                k = -k * 0.5\n",
    "                            n += weight * k\n",
    "                            m += weight\n",
    "                            break\n",
    "            # Numbers, citations, explanations make the sentence more factual.\n",
    "            if w.type in (\"CD\", \"\\\"\", \"'\", \":\", \"(\"):\n",
    "                n += 0.75\n",
    "                m += 1\n",
    "    if m == 0:\n",
    "        return 1.0 # No modal verbs/adverbs used, so statement must be true.\n",
    "    return max(-1.0, min(n / (m or 1), +1.0))\n",
    "\n",
    "\n",
    "def uncertain(sentence, threshold=0.5):\n",
    "    return modality(sentence) <= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(embed_dir=EMB_PATH):\n",
    "    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in tqdm(open(embed_dir)))\n",
    "    return embedding_index\n",
    "\n",
    "def build_embedding_matrix(word_index, embeddings_index, MAX_FEATURES, lower = True, verbose = True):\n",
    "    embedding_matrix = np.zeros((MAX_FEATURES, EMBED_SIZE))\n",
    "    for word, i in tqdm(word_index.items(),disable = not verbose):\n",
    "        if lower:\n",
    "            word = word.lower()\n",
    "        if i >= MAX_FEATURES: continue\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_vector = embeddings_index[\"unknown\"]\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "def build_matrix(word_index, embeddings_index):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_matrix[i] = embeddings_index[\"unknown\"]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_FEATURES, lower=True)\n",
    "tokenizer.fit_on_texts(list(train_df[TEXT_COL]) + list(test_df[TEXT_COL]))\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# keys = list(word_index.keys())\n",
    "# for key in keys:\n",
    "    # correct_key = correction(key)\n",
    "    # if correct_key != key and correct_key in keys:\n",
    "        # word_index[key] = word_index[correct_key]\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(list(train_df[TEXT_COL]))\n",
    "X_test = tokenizer.texts_to_sequences(list(test_df[TEXT_COL]))\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=MAXLEN)\n",
    "X_test = pad_sequences(X_test, maxlen=MAXLEN)\n",
    "\n",
    "y_train = train_df['target'].values\n",
    "\n",
    "del tokenizer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000001it [03:12, 10391.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index = load_embeddings()\n",
    "embedding_matrix = build_matrix(word_index, embeddings_index)\n",
    "del embeddings_index\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/bfelbo/DeepMoji/blob/master/deepmoji/attlayer.py\n",
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Capsule(Layer):\n",
    "    \"\"\"Capsule Layer implementation in Keras\n",
    "       This implementation is based on Dynamic Routing of Capsules,\n",
    "       Geoffrey Hinton et. al.\n",
    "       The Capsule Layer is a Neural Network Layer which helps\n",
    "       modeling relationships in image and sequential data better\n",
    "       than just CNNs or RNNs. It achieves this by understanding\n",
    "       the spatial relationships between objects (in images)\n",
    "       or words (in text) by encoding additional information\n",
    "       about the image or text, such as angle of rotation,\n",
    "       thickness and brightness, relative proportions etc.\n",
    "       This layer can be used instead of pooling layers to\n",
    "       lower dimensions and still capture important information\n",
    "       about the relationships and structures within the data.\n",
    "       A normal pooling layer would lose a lot of\n",
    "       this information.\n",
    "       This layer can be used on the output of any layer\n",
    "       which has a 3-D output (including batch_size). For example,\n",
    "       in image classification, it can be used on the output of a\n",
    "       Conv2D layer for Computer Vision applications. Also,\n",
    "       it can be used on the output of a GRU or LSTM Layer\n",
    "       (Bidirectional or Unidirectional) for NLP applications.\n",
    "       The default activation function is 'linear'. But, this layer\n",
    "       is generally used with the 'squash' activation function\n",
    "       (recommended). To use the squash activation function, do :\n",
    "       from keras_contrib.activations import squash\n",
    "       capsule = Capsule(num_capsule=10,\n",
    "                         dim_capsule=10,\n",
    "                         routings=3,\n",
    "                         share_weights=True,\n",
    "                         activation=squash)\n",
    "       # Example usage :\n",
    "           1). COMPUTER VISION\n",
    "           input_image = Input(shape=(None, None, 3))\n",
    "           conv_2d = Conv2D(64,\n",
    "                            (3, 3),\n",
    "                            activation='relu')(input_image)\n",
    "           capsule = Capsule(num_capsule=10,\n",
    "                             dim_capsule=16,\n",
    "                             routings=3,\n",
    "                             activation='relu',\n",
    "                             share_weights=True)(conv_2d)\n",
    "           2). NLP\n",
    "           maxlen = 72\n",
    "           max_features = 120000\n",
    "           input_text = Input(shape=(maxlen,))\n",
    "           embedding = Embedding(max_features,\n",
    "                                 embed_size,\n",
    "                                 weights=[embedding_matrix],\n",
    "                                 trainable=False)(input_text)\n",
    "           bi_gru = Bidirectional(GRU(64,\n",
    "                                      return_seqeunces=True))(embedding)\n",
    "           capsule = Capsule(num_capsule=5,\n",
    "                             dim_capsule=5,\n",
    "                             routings=4,\n",
    "                             activation='sigmoid',\n",
    "                             share_weights=True)(bi_gru)\n",
    "       # Arguments\n",
    "           num_capsule : Number of Capsules (int)\n",
    "           dim_capsules : Dimensions of the vector output of each Capsule (int)\n",
    "           routings : Number of dynamic routings in the Capsule Layer (int)\n",
    "           share_weights : Whether to share weights between Capsules or not\n",
    "           (boolean)\n",
    "           activation : Activation function for the Capsules\n",
    "           regularizer : Regularizer for the weights of the Capsules\n",
    "           initializer : Initializer for the weights of the Caspules\n",
    "           constraint : Constraint for the weights of the Capsules\n",
    "       # Input shape\n",
    "            3D tensor with shape:\n",
    "            (batch_size, input_num_capsule, input_dim_capsule)\n",
    "            [any 3-D Tensor with the first dimension as batch_size]\n",
    "       # Output shape\n",
    "            3D tensor with shape:\n",
    "            (batch_size, num_capsule, dim_capsule)\n",
    "       # References\n",
    "        - [Dynamic-Routing-Between-Capsules]\n",
    "          (https://arxiv.org/pdf/1710.09829.pdf)\n",
    "        - [Keras-Examples-CIFAR10-CNN-Capsule]\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_capsule,\n",
    "                 dim_capsule,\n",
    "                 routings=3,\n",
    "                 share_weights=True,\n",
    "                 initializer='glorot_uniform',\n",
    "                 activation=None,\n",
    "                 regularizer=None,\n",
    "                 constraint=None,\n",
    "                 **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.share_weights = share_weights\n",
    "\n",
    "        self.activation = activations.get(activation)\n",
    "        self.regularizer = regularizers.get(regularizer)\n",
    "        self.initializer = initializers.get(initializer)\n",
    "        self.constraint = constraints.get(constraint)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule *\n",
    "                                            self.dim_capsule),\n",
    "                                     initializer=self.initializer,\n",
    "                                     regularizer=self.regularizer,\n",
    "                                     constraint=self.constraint,\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule *\n",
    "                                            self.dim_capsule),\n",
    "                                     initializer=self.initializer,\n",
    "                                     regularizer=self.regularizer,\n",
    "                                     constraint=self.constraint,\n",
    "                                     trainable=True)\n",
    "\n",
    "        self.build = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vectors = K.conv1d(inputs, self.W)\n",
    "        else:\n",
    "            u_hat_vectors = K.local_conv1d(inputs, self.W, [1], [1])\n",
    "\n",
    "        # u_hat_vectors : The spatially transformed input vectors (with local_conv_1d)\n",
    "\n",
    "        batch_size = K.shape(inputs)[0]\n",
    "        input_num_capsule = K.shape(inputs)[1]\n",
    "        u_hat_vectors = K.reshape(u_hat_vectors, (batch_size,\n",
    "                                                  input_num_capsule,\n",
    "                                                  self.num_capsule,\n",
    "                                                  self.dim_capsule))\n",
    "\n",
    "        u_hat_vectors = K.permute_dimensions(u_hat_vectors, (0, 2, 1, 3))\n",
    "        routing_weights = K.zeros_like(u_hat_vectors[:, :, :, 0])\n",
    "\n",
    "        for i in range(self.routings):\n",
    "            capsule_weights = K.softmax(routing_weights, 1)\n",
    "            outputs = K.batch_dot(capsule_weights, u_hat_vectors, [2, 2])\n",
    "            if K.ndim(outputs) == 4:\n",
    "                outputs = K.sum(outputs, axis=1)\n",
    "            if i < self.routings - 1:\n",
    "                outputs = K.l2_normalize(outputs, -1)\n",
    "                routing_weights = K.batch_dot(outputs, u_hat_vectors, [2, 3])\n",
    "                if K.ndim(routing_weights) == 4:\n",
    "                    routing_weights = K.sum(routing_weights, axis=1)\n",
    "\n",
    "        return self.activation(outputs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'num_capsule': self.num_capsule,\n",
    "                  'dim_capsule': self.dim_capsule,\n",
    "                  'routings': self.routings,\n",
    "                  'share_weights': self.share_weights,\n",
    "                  'activation': activations.serialize(self.activation),\n",
    "                  'regularizer': regularizers.serialize(self.regularizer),\n",
    "                  'initializer': initializers.serialize(self.initializer),\n",
    "                  'constraint': constraints.serialize(self.constraint)}\n",
    "\n",
    "        base_config = super(Capsule, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    \"\"\"\n",
    "    Squash activation function (generally used in Capsule layers).\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    scale = K.sqrt(s_squared_norm) / (0.5 + s_squared_norm)\n",
    "    return scale * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(MAXLEN,))\n",
    "    embed_inp = Embedding(len(word_index) + 1, EMBED_SIZE, weights=[embedding_matrix], input_length=MAXLEN, trainable=False)(inp)\n",
    "    embed_inp = SpatialDropout1D(0.2)(embed_inp)\n",
    "    \n",
    "    bi_lstm = Bidirectional(CuDNNLSTM(64, return_sequences=True))(embed_inp)\n",
    "    # bi_gru = Bidirectional(CuDNNGRU(96, return_sequences=True))(bi_lstm)\n",
    "    \n",
    "    max_pool = GlobalMaxPooling1D()(bi_lstm)\n",
    "    avg_pool = GlobalAveragePooling1D()(bi_lstm)\n",
    "    attention = AttentionWeightedAverage()(bi_lstm)\n",
    "    # capsule = Capsule(num_capsule=10, dim_capsule=10, routings=4, activation=squash)(bi_gru)\n",
    "    # capsule = Flatten()(capsule)\n",
    "    \n",
    "    x = concatenate([max_pool, avg_pool, attention], axis=1)\n",
    "    # x = Dropout(rate=0.1)(x)\n",
    "    # x = Dense(16)(x)\n",
    "    outp = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inp, outp)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.005, decay=0.001), metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "sample_model  = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 220)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 220, 300)     123014100   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 220, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 220, 128)     187392      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_weighted_average_1 (A (None, 128)          128         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 384)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 attention_weighted_average_1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            385         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 123,202,005\n",
      "Trainable params: 187,905\n",
      "Non-trainable params: 123,014,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sample_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"483pt\" viewBox=\"0.00 0.00 992.00 483.00\" width=\"992pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 479)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-479 988,-479 988,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139862018500760 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139862018500760</title>\n",
       "<polygon fill=\"none\" points=\"401.5,-438.5 401.5,-474.5 526.5,-474.5 526.5,-438.5 401.5,-438.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"464\" y=\"-452.8\">input_1: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139862018501600 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139862018501600</title>\n",
       "<polygon fill=\"none\" points=\"383.5,-365.5 383.5,-401.5 544.5,-401.5 544.5,-365.5 383.5,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"464\" y=\"-379.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 139862018500760&#45;&gt;139862018501600 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139862018500760-&gt;139862018501600</title>\n",
       "<path d=\"M464,-438.313C464,-430.289 464,-420.547 464,-411.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"467.5,-411.529 464,-401.529 460.5,-411.529 467.5,-411.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139862018502496 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139862018502496</title>\n",
       "<polygon fill=\"none\" points=\"347,-292.5 347,-328.5 581,-328.5 581,-292.5 347,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"464\" y=\"-306.8\">spatial_dropout1d_1: SpatialDropout1D</text>\n",
       "</g>\n",
       "<!-- 139862018501600&#45;&gt;139862018502496 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139862018501600-&gt;139862018502496</title>\n",
       "<path d=\"M464,-365.313C464,-357.289 464,-347.547 464,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"467.5,-338.529 464,-328.529 460.5,-338.529 467.5,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139862018949192 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139862018949192</title>\n",
       "<polygon fill=\"none\" points=\"287.5,-219.5 287.5,-255.5 640.5,-255.5 640.5,-219.5 287.5,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"464\" y=\"-233.8\">bidirectional_1(cu_dnnlstm_1): Bidirectional(CuDNNLSTM)</text>\n",
       "</g>\n",
       "<!-- 139862018502496&#45;&gt;139862018949192 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139862018502496-&gt;139862018949192</title>\n",
       "<path d=\"M464,-292.313C464,-284.289 464,-274.547 464,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"467.5,-265.529 464,-255.529 460.5,-265.529 467.5,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139862021040280 -->\n",
       "<g class=\"node\" id=\"node5\"><title>139862021040280</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 284,-182.5 284,-146.5 0,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"142\" y=\"-160.8\">global_max_pooling1d_1: GlobalMaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 139862018949192&#45;&gt;139862021040280 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>139862018949192-&gt;139862021040280</title>\n",
       "<path d=\"M386.873,-219.494C339.44,-209.035 278.533,-195.605 229.329,-184.756\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"229.853,-181.287 219.334,-182.552 228.346,-188.123 229.853,-181.287\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139862021040336 -->\n",
       "<g class=\"node\" id=\"node6\"><title>139862021040336</title>\n",
       "<polygon fill=\"none\" points=\"302.5,-146.5 302.5,-182.5 625.5,-182.5 625.5,-146.5 302.5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"464\" y=\"-160.8\">global_average_pooling1d_1: GlobalAveragePooling1D</text>\n",
       "</g>\n",
       "<!-- 139862018949192&#45;&gt;139862021040336 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>139862018949192-&gt;139862021040336</title>\n",
       "<path d=\"M464,-219.313C464,-211.289 464,-201.547 464,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"467.5,-192.529 464,-182.529 460.5,-192.529 467.5,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139862021040224 -->\n",
       "<g class=\"node\" id=\"node7\"><title>139862021040224</title>\n",
       "<polygon fill=\"none\" points=\"644,-146.5 644,-182.5 984,-182.5 984,-146.5 644,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"814\" y=\"-160.8\">attention_weighted_average_1: AttentionWeightedAverage</text>\n",
       "</g>\n",
       "<!-- 139862018949192&#45;&gt;139862021040224 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>139862018949192-&gt;139862021040224</title>\n",
       "<path d=\"M547.834,-219.494C599.714,-208.969 666.425,-195.437 720.081,-184.552\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"720.837,-187.97 729.941,-182.552 719.445,-181.11 720.837,-187.97\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139861543975792 -->\n",
       "<g class=\"node\" id=\"node8\"><title>139861543975792</title>\n",
       "<polygon fill=\"none\" points=\"380,-73.5 380,-109.5 548,-109.5 548,-73.5 380,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"464\" y=\"-87.8\">concatenate_1: Concatenate</text>\n",
       "</g>\n",
       "<!-- 139862021040280&#45;&gt;139861543975792 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>139862021040280-&gt;139861543975792</title>\n",
       "<path d=\"M219.127,-146.494C266.56,-136.035 327.467,-122.605 376.671,-111.756\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"377.654,-115.123 386.666,-109.552 376.147,-108.287 377.654,-115.123\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139862021040336&#45;&gt;139861543975792 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>139862021040336-&gt;139861543975792</title>\n",
       "<path d=\"M464,-146.313C464,-138.289 464,-128.547 464,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"467.5,-119.529 464,-109.529 460.5,-119.529 467.5,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139862021040224&#45;&gt;139861543975792 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>139862021040224-&gt;139861543975792</title>\n",
       "<path d=\"M730.166,-146.494C678.286,-135.969 611.575,-122.437 557.919,-111.552\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"558.555,-108.11 548.059,-109.552 557.163,-114.97 558.555,-108.11\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139862074893256 -->\n",
       "<g class=\"node\" id=\"node9\"><title>139862074893256</title>\n",
       "<polygon fill=\"none\" points=\"413,-0.5 413,-36.5 515,-36.5 515,-0.5 413,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"464\" y=\"-14.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 139861543975792&#45;&gt;139862074893256 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>139861543975792-&gt;139862074893256</title>\n",
       "<path d=\"M464,-73.3129C464,-65.2895 464,-55.5475 464,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"467.5,-46.5288 464,-36.5288 460.5,-46.5289 467.5,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(sample_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 1443899 samples, validate on 360975 samples\n",
      "Epoch 1/100\n",
      "1443899/1443899 [==============================] - 142s 98us/step - loss: 0.1193 - acc: 0.9568 - val_loss: 0.0923 - val_acc: 0.9648\n",
      "Epoch 2/100\n",
      "1443899/1443899 [==============================] - 139s 96us/step - loss: 0.0939 - acc: 0.9642 - val_loss: 0.0893 - val_acc: 0.9658\n",
      "Epoch 3/100\n",
      "1443899/1443899 [==============================] - 139s 96us/step - loss: 0.0897 - acc: 0.9656 - val_loss: 0.0886 - val_acc: 0.9657\n",
      "Epoch 4/100\n",
      "1443899/1443899 [==============================] - 139s 96us/step - loss: 0.0873 - acc: 0.9664 - val_loss: 0.0893 - val_acc: 0.9650\n",
      "Epoch 5/100\n",
      "1443899/1443899 [==============================] - 139s 96us/step - loss: 0.0854 - acc: 0.9670 - val_loss: 0.0863 - val_acc: 0.9669\n",
      "Epoch 6/100\n",
      "1443899/1443899 [==============================] - 139s 97us/step - loss: 0.0838 - acc: 0.9677 - val_loss: 0.0864 - val_acc: 0.9668\n",
      "Epoch 7/100\n",
      "1443899/1443899 [==============================] - 139s 97us/step - loss: 0.0827 - acc: 0.9680 - val_loss: 0.0870 - val_acc: 0.9661\n",
      "Epoch 8/100\n",
      "1443899/1443899 [==============================] - 140s 97us/step - loss: 0.0816 - acc: 0.9684 - val_loss: 0.0862 - val_acc: 0.9668\n",
      "Epoch 9/100\n",
      "1443899/1443899 [==============================] - 140s 97us/step - loss: 0.0805 - acc: 0.9687 - val_loss: 0.0869 - val_acc: 0.9663\n",
      "Epoch 10/100\n",
      "1443899/1443899 [==============================] - 139s 96us/step - loss: 0.0796 - acc: 0.9692 - val_loss: 0.0866 - val_acc: 0.9664\n",
      "Epoch 11/100\n",
      "1443899/1443899 [==============================] - 139s 96us/step - loss: 0.0788 - acc: 0.9693 - val_loss: 0.0867 - val_acc: 0.9666\n",
      "Epoch 00011: early stopping\n",
      "Train on 1443899 samples, validate on 360975 samples\n",
      "Epoch 1/100\n",
      "1443899/1443899 [==============================] - 140s 97us/step - loss: 0.1165 - acc: 0.9584 - val_loss: 0.0932 - val_acc: 0.9650\n",
      "Epoch 2/100\n",
      "1443899/1443899 [==============================] - 140s 97us/step - loss: 0.0923 - acc: 0.9647 - val_loss: 0.0912 - val_acc: 0.9644\n",
      "Epoch 3/100\n",
      "1443899/1443899 [==============================] - 140s 97us/step - loss: 0.0883 - acc: 0.9661 - val_loss: 0.0869 - val_acc: 0.9666\n",
      "Epoch 4/100\n",
      "1443899/1443899 [==============================] - 140s 97us/step - loss: 0.0859 - acc: 0.9669 - val_loss: 0.0868 - val_acc: 0.9659\n",
      "Epoch 5/100\n",
      "1443899/1443899 [==============================] - 139s 96us/step - loss: 0.0841 - acc: 0.9675 - val_loss: 0.0894 - val_acc: 0.9645\n",
      "Epoch 6/100\n",
      " 497664/1443899 [=========>....................] - ETA: 1:23 - loss: 0.0827 - acc: 0.9681"
     ]
    }
   ],
   "source": [
    "splits = list(KFold(n_splits=5).split(X_train,y_train))\n",
    "\n",
    "oof_preds = np.zeros((X_train.shape[0]))\n",
    "test_preds = np.zeros((X_test.shape[0]))\n",
    "\n",
    "for fold in [0, 1, 2, 3, 4]:\n",
    "    K.clear_session()\n",
    "    tr_ind, val_ind = splits[fold]\n",
    "\n",
    "    ckpt = ModelCheckpoint(f'gru_{fold}.hdf5', save_best_only = True)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "    model = get_model()\n",
    "\n",
    "    model.fit(X_train[tr_ind],\n",
    "              y_train[tr_ind]>0.5,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=NUM_EPOCHS,\n",
    "              validation_data=(X_train[val_ind], y_train[val_ind]>0.5),\n",
    "              callbacks = [es,ckpt])\n",
    "\n",
    "    oof_preds[val_ind] += model.predict(X_train[val_ind])[:,0]\n",
    "    test_preds += model.predict(X_test)[:,0]\n",
    "\n",
    "test_preds /= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000000</td>\n",
       "      <td>0.000426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7000001</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7000002</td>\n",
       "      <td>0.003385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7000003</td>\n",
       "      <td>0.000786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000004</td>\n",
       "      <td>0.957688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  prediction\n",
       "0  7000000    0.000426\n",
       "1  7000001    0.000039\n",
       "2  7000002    0.003385\n",
       "3  7000003    0.000786\n",
       "4  7000004    0.957688"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')\n",
    "submission['prediction'] = test_preds\n",
    "submission.reset_index(drop=False, inplace=True)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
